# -*- coding: utf-8 -*-
"""APPLE .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jHmTXzYfJ7ngI9aQWX4mutMREM_W2MsF

## Ingesting PDF
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --q unstructured langchain
# %pip install --q "unstructured[all-docs]"

!pip install langchain
!pip install langchain-community
!pip install unstructured

from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.document_loaders import OnlinePDFLoader

# List of PDF file paths
pdf_paths = [
    "/home/goog-10-k-2023 (1).pdf",
    "/home/tsla-20231231-gen.pdf",
    "/home/uber-10-k-2023.pdf"
]

# Placeholder for the loaded data
all_data = []

# Iterate over the PDF paths and load each file
for path in pdf_paths:
    loader = UnstructuredPDFLoader(file_path=path)
    data = loader.load()
    all_data.append(data)

# Process the loaded data as needed
for data in all_data:
    print(data)

from langchain.document_loaders import DirectoryLoader

directory = '/home'

def load_docs(directory):
    loader = DirectoryLoader(directory)
    documents = loader.load()
    return documents

documents = load_docs(directory)
print(len(documents))

from langchain.text_splitter import RecursiveCharacterTextSplitter

def split_docs(documents,chunk_size=1000,chunk_overlap=20):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
  docs = text_splitter.split_documents(documents)
  return docs

docs = split_docs(documents)
print(len(docs))

print(docs[0].page_content)

#requires for open ai embedding
!pip install tiktoken -q

!pip install -U langchain-openai

!pip install --upgrade openai

import openai
from langchain.embeddings.openai import OpenAIEmbeddings

# Ensure the OpenAI API key is set
import os
os.environ["OPENAI_API_KEY"] = "sk-proj-hVHrOODEvdzbfLMTRDjkT3BlbkFJzj2hqZnhgtgr4R8Yyf1B"

# Correct initialization of OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# Embed a sample query
query_result = embeddings.embed_query("Hello world")

# Print the length of the embedding result
print(len(query_result))

!pip install pinecone-client -q
!pip install --upgrade pinecone-client langchain

import os
from pinecone import Pinecone, ServerlessSpec
from langchain.vectorstores import Pinecone as LangchainPinecone

# Set your Pinecone API key and environment
os.environ['PINECONE_API_KEY'] = '2de62d78-857d-42a6-810e-0024cd5ce59d'

# Initialize Pinecone
pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])

# Define the index name
index_name = "langchain-demo"

# Check if the index exists, create if not
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # Adjust this dimension according to your embeddings
        metric='cosine',  # Change the metric as needed
        spec=ServerlessSpec(
            cloud='aws',
            region='us-east-1'
        )
    )

# Assuming 'docs' and 'embeddings' are already defined
index = LangchainPinecone.from_documents(docs, embeddings, index_name=index_name)

def get_similar_docs(query, k=2, score=False):
    if score:
        similar_docs = index.similarity_search_with_score(query, k=k)
    else:
        similar_docs = index.similarity_search(query, k=k)
    return similar_docs, score

query = "What is Segment Information Of Tesla"
similar_docs, score = get_similar_docs(query)

# Display the results in a readable format
for i, doc in enumerate(similar_docs):
    if score:
        document, similarity_score = doc
        print(f"Document {i+1}:")
        print(f"Source: {document.metadata['source']}")
        print(f"Score: {similarity_score}")
        print(f"Content Preview: {document.page_content[:500]}")
    else:
        print(f"Document {i+1}:")
        print(f"Source: {doc.metadata['source']}")
        print(f"Content Preview: {doc.page_content[:500]}")
    print("\n" + "="*80 + "\n")

import openai
from langchain.llms import OpenAI
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-proj-hVHrOODEvdzbfLMTRDjkT3BlbkFJzj2hqZnhgtgr4R8Yyf1B"

# Initialize the OpenAI language model
model_name = "gpt-3.5-turbo"
llm = OpenAI(model_name=model_name, openai_api_key=os.getenv("OPENAI_API_KEY"))

!pip install faiss-cpu

!pip install streamlit

import streamlit as st

from langchain.chains.question_answering import load_qa_chain
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-proj-hVHrOODEvdzbfLMTRDjkT3BlbkFJzj2hqZnhgtgr4R8Yyf1B"

# Initialize the language model
llm = OpenAI(temperature=0)

# Load the QA chain
chain = load_qa_chain(llm, chain_type="stuff")

# Initialize the embedding model
embeddings = OpenAIEmbeddings()

# Load and process the documents
# For this example, we'll create a simple text file
with open('sample.txt', 'w') as f:
    f.write("Tesla's segment information includes automotive sales and leasing, energy generation and storage, and services and other. The automotive segment is the largest, comprising electric vehicle sales and regulatory credits.")

loader = TextLoader('sample.txt')
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Create the vector store
index = FAISS.from_documents(texts, embeddings)

# Function to get similar documents
def get_similar_docs(query, k=2, score=False):
    if score:
        similar_docs = index.similarity_search_with_score(query, k=k)
    else:
        similar_docs = index.similarity_search(query, k=k)
    return similar_docs

# Function to get the answer to a query
def get_answer(query):
    similar_docs = get_similar_docs(query)
    answer = chain({"input_documents": similar_docs, "question": query})
    return answer['output_text']

# Example usage
query = "What is Segment Information Of Tesla"
answer = get_answer(query)
print(answer)



